{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xiaofanpaiooo-code/whisper-lora/blob/xiaofanpaiooo-code-patch-1/whisper-2.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 部署说明：\n",
        "# 1. 复制此代码到 Colab。\n",
        "# 2. 运行。\n",
        "# 3. 这次使用了 Popen 管道直连，必须能看到输出！\n",
        "# ==========================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# 1. 环境安装\n",
        "def install_dependencies():\n",
        "    packages = [\"faster-whisper\", \"fastapi\", \"uvicorn\", \"python-multipart\", \"pyngrok\"]\n",
        "    # -q 表示静默安装\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + packages)\n",
        "\n",
        "try:\n",
        "    import faster_whisper\n",
        "    import fastapi\n",
        "    from pyngrok import ngrok\n",
        "    import uvicorn\n",
        "except ImportError:\n",
        "    print(\"正在安装依赖环境，请稍候...\", flush=True)\n",
        "    install_dependencies()\n",
        "    print(\"环境安装完成！\", flush=True)\n",
        "\n",
        "# ================= 配置区域 =================\n",
        "NGROK_AUTH_TOKEN = \"37hXaj4l3VhFmrso4FTbEWYE3Li_7Z3vCLhdMAPb2BpiVufAw\"\n",
        "# ===========================================\n",
        "\n",
        "# 2. 生成独立服务脚本 (包含端口清理功能)\n",
        "server_code = f\"\"\"\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import time\n",
        "import subprocess\n",
        "from fastapi import FastAPI, UploadFile, File\n",
        "from fastapi.responses import JSONResponse\n",
        "from faster_whisper import WhisperModel\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import torch\n",
        "\n",
        "# 强制刷新打印函数\n",
        "def log(msg):\n",
        "    print(msg, flush=True)\n",
        "\n",
        "# 杀掉占用 8000 端口的旧进程\n",
        "def kill_port(port):\n",
        "    try:\n",
        "        command = f\"fuser -k {{port}}/tcp\"\n",
        "        subprocess.run(command, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "        time.sleep(1)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "NGROK_AUTH_TOKEN = \"{NGROK_AUTH_TOKEN}\"\n",
        "\n",
        "kill_port(8000)\n",
        "app = FastAPI()\n",
        "\n",
        "# 检查 GPU\n",
        "if not torch.cuda.is_available():\n",
        "    log(\"\\\\n❌ 严重错误：未检测到 GPU！\")\n",
        "    log(\"请在顶部菜单点击：[代码执行程序] -> [更改运行时类型] -> 选择 T4 GPU\\\\n\")\n",
        "    sys.exit(1)\n",
        "\n",
        "log(\"正在加载 Whisper 模型 (Tiny + float16)...\")\n",
        "try:\n",
        "    model = WhisperModel(\"tiny\", device=\"cuda\", compute_type=\"float16\")\n",
        "    log(\"✅ 模型加载完毕！\")\n",
        "except Exception as e:\n",
        "    log(f\"❌ 模型加载失败: {{e}}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "@app.post(\"/transcribe\")\n",
        "async def transcribe(file: UploadFile = File(...)):\n",
        "    start_time = time.time()\n",
        "    temp_filename = f\"temp_{{file.filename}}\"\n",
        "    try:\n",
        "        with open(temp_filename, \"wb\") as buffer:\n",
        "            shutil.copyfileobj(file.file, buffer)\n",
        "\n",
        "        segments, info = model.transcribe(temp_filename, beam_size=1, language=\"zh\")\n",
        "        text_result = \"\".join([segment.text for segment in segments])\n",
        "\n",
        "        log(f\"耗时: {{time.time() - start_time:.2f}}s | 结果: {{text_result}}\")\n",
        "        return JSONResponse(content={{\"text\": text_result.strip()}})\n",
        "    except Exception as e:\n",
        "        log(f\"❌ 推理出错: {{e}}\")\n",
        "        return JSONResponse(content={{\"error\": str(e)}}, status_code=500)\n",
        "    finally:\n",
        "        if os.path.exists(temp_filename):\n",
        "            os.remove(temp_filename)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if not NGROK_AUTH_TOKEN:\n",
        "        log(\"❌ 错误：Token 未设置\")\n",
        "    else:\n",
        "        ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "        try:\n",
        "            public_url = ngrok.connect(8000).public_url\n",
        "            log(f\"\\\\n======== 服务已启动 ========\")\n",
        "            log(f\"API 接口地址 (请复制到客户端): {{public_url}}/transcribe\")\n",
        "            log(f\"===========================\\\\n\")\n",
        "            uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "        except Exception as e:\n",
        "            log(f\"❌ 启动失败: {{e}}\")\n",
        "\"\"\"\n",
        "\n",
        "with open(\"server_main.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(server_code)\n",
        "\n",
        "print(\"已生成服务脚本，正在启动...\", flush=True)\n",
        "\n",
        "# 3. 使用 Popen + 实时管道读取 (终极解决方案)\n",
        "# 这能确保哪怕是一个字符的输出，也能立刻被抓取显示\n",
        "env = os.environ.copy()\n",
        "env[\"PYTHONUNBUFFERED\"] = \"1\"\n",
        "\n",
        "process = subprocess.Popen(\n",
        "    [sys.executable, \"server_main.py\"],\n",
        "    env=env,\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT, # 把错误信息也合并到输出流\n",
        "    text=True,\n",
        "    bufsize=1 # 行缓冲\n",
        ")\n",
        "\n",
        "# 实时读取输出并打印\n",
        "try:\n",
        "    for line in process.stdout:\n",
        "        print(line, end=\"\")\n",
        "except KeyboardInterrupt:\n",
        "    print(\"服务已停止\")\n",
        "    process.terminate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBp4TEJRVKHw",
        "outputId": "ab4f1ef7-2130-45f6-e0ab-2bac5f285b44"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "已生成服务脚本，正在启动...\n",
            "正在加载 Whisper 模型 (Tiny + float16)...\n",
            "✅ 模型加载完毕！\n",
            "\n",
            "======== 服务已启动 ========\n",
            "API 接口地址 (请复制到客户端): https://unavoidable-amber-plinthlike.ngrok-free.dev/transcribe\n",
            "===========================\n",
            "\n",
            "INFO:     Started server process [12274]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
            "耗时: 2.92s | 结果: 這才就有個影片已有機會做到以後我才跟過了但從 2009年大陸的年輕人會打到此時的大陸獵人一般的一般一般的一般一般的一般一般的一般一般的一般\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 3.00s | 结果: 在這裏的風 跟著其他人的大人很適合在這裏的小子和小子的小子的小子和小子的小子的小子的小子和小子的小子的小子的小子\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 0.72s | 结果: 本身是Movie Bronson 2009所以其實其實其實是Cling Charles Bronson\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 2.10s | 结果: 突然是能發現他喜歡蜜蜜就是一位康廷的朋友和光珍紋的時間就算好好玩\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 1.49s | 结果: 他有一個人的心情他在一邊都在這兒他在這兒都在這兒他在這兒都在這兒然後他在這兒他在這兒都在這兒他在這兒都在這兒\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 2.64s | 结果: 這案不好才重點所以我告訴你們這兩個地方小點都沒有辦法不過在這直播這只是一個浮影他們覺得我還沒用到我的蜂蜜\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 1.85s | 结果: wards the rivers were rising and rising and rising,and it was a boy who'd had his foot stuck in a great,and the rivers rose and rose, that were people's help.\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 0.48s | 结果: 把水被冰淇淋了把水被冰淇淋了\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 1.24s | 结果: 我會告訴他們,他在一起他在一起,他在一起他在一起,他在一起\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 1.09s | 结果: 就像你做了一切所以你能否做了一切做了一切做了一切\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 0.89s | 结果: 你再說一句\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 1.68s | 结果: 我現在要請大家通過各種經濟的影響和美日的經濟和美日的經濟和美日的經濟是非常感謝\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 2.15s | 结果: 本身有可能是否能不能有很多事情我真的非常有趣的我曾經在這兒拍攝了一件事所以,現在大家可以說在這兒拍攝這件事我可以說\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 1.65s | 结果: 你会想想起来的一种小声音的歌曲其实,你会想起来的歌曲我会想起来的歌曲\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 1.81s | 结果: 而是否有超過一部人,更有感受到的問題,因此是一部分人在你家裡找到你的問題。這就是這個 video讓我做到的表現,所有的意義並沒有過得很明顯。\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 1.85s | 结果: 1990年, 而是有了有了很多的有了很多的有了很多的我看了一段時間他在他在他身上在他身上在他身上在他身上在他身上在他身上在他身上\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 0.20s | 结果: South London criminal who is incarcerated who still isn't incarcerated as far as I know and Tom played Charles Bronson in the movie Bronson 2009 and so obviously before actually\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 0.74s | 结果: ClankshelvesBones and me\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 1.83s | 结果: he had an encounter i e he needed to meet himand Charles was actually the one who called Tom on the phoneand courtam at a bad timeTom had just been going through a really\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 2.23s | 结果: 我想要和你在我做上面慢慢的面對是很適合套互繪 bending因為他們真的身為很多人都在中心所以他曾有這樣的不一樣因為是這樣的對界部份對界部份對界部份對界部份對界部份對界部份用好一隻會互繪剛才看得不到只為了同體一個很貼心因為真的很好所以不然是嗎\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 1.70s | 结果: 很恐怖的專家所以後來說他們說這個專家這就是專家的專家這就是專家的專家然後他們說這個專家\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "服务已停止\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}