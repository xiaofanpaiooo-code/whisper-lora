{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xiaofanpaiooo-code/whisper-lora/blob/xiaofanpaiooo-code-patch-1/whisper_zimu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 部署说明：\n",
        "# 1. 复制此代码到 Colab。\n",
        "# 2. 运行。\n",
        "# 3. 英文专用版：听英文 -> 写英文 (速度极快)。\n",
        "# ==========================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# 1. 环境安装\n",
        "def install_dependencies():\n",
        "    packages = [\"faster-whisper\", \"fastapi\", \"uvicorn\", \"python-multipart\", \"pyngrok\"]\n",
        "    # -q 表示静默安装\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + packages)\n",
        "\n",
        "try:\n",
        "    import faster_whisper\n",
        "    import fastapi\n",
        "    from pyngrok import ngrok\n",
        "    import uvicorn\n",
        "except ImportError:\n",
        "    print(\"正在安装依赖环境，请稍候...\", flush=True)\n",
        "    install_dependencies()\n",
        "    print(\"环境安装完成！\", flush=True)\n",
        "\n",
        "# ================= 配置区域 =================\n",
        "NGROK_AUTH_TOKEN = \"37hXaj4l3VhFmrso4FTbEWYE3Li_7Z3vCLhdMAPb2BpiVufAw\"\n",
        "# ===========================================\n",
        "\n",
        "# 2. 生成独立服务脚本\n",
        "server_code = f\"\"\"\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import time\n",
        "import subprocess\n",
        "from fastapi import FastAPI, UploadFile, File\n",
        "from fastapi.responses import JSONResponse\n",
        "from faster_whisper import WhisperModel\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import torch\n",
        "\n",
        "def log(msg):\n",
        "    print(msg, flush=True)\n",
        "\n",
        "def kill_port(port):\n",
        "    try:\n",
        "        command = f\"fuser -k {{port}}/tcp\"\n",
        "        subprocess.run(command, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "        time.sleep(1)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "NGROK_AUTH_TOKEN = \"{NGROK_AUTH_TOKEN}\"\n",
        "\n",
        "kill_port(8000)\n",
        "app = FastAPI()\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    log(\"\\\\n❌ 严重错误：未检测到 GPU！请切换运行时类型为 T4 GPU。\\\\n\")\n",
        "    sys.exit(1)\n",
        "\n",
        "log(\"正在加载 Whisper 模型 (Tiny + float16)...\")\n",
        "try:\n",
        "    # 英文场景下 tiny 模型效果惊人地好，且速度最快\n",
        "    model = WhisperModel(\"tiny\", device=\"cuda\", compute_type=\"float16\")\n",
        "    log(\"✅ 模型加载完毕！\")\n",
        "except Exception as e:\n",
        "    log(f\"❌ 模型加载失败: {{e}}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "@app.post(\"/transcribe\")\n",
        "async def transcribe(file: UploadFile = File(...)):\n",
        "    start_time = time.time()\n",
        "    temp_filename = f\"temp_{{file.filename}}\"\n",
        "    try:\n",
        "        with open(temp_filename, \"wb\") as buffer:\n",
        "            shutil.copyfileobj(file.file, buffer)\n",
        "\n",
        "        # ========================================================\n",
        "        # 英文专用配置\n",
        "        # ========================================================\n",
        "        segments, info = model.transcribe(\n",
        "            temp_filename,\n",
        "            beam_size=1,\n",
        "            best_of=1,\n",
        "            language=\"en\",     # <--- 关键修改：指定为英语\n",
        "\n",
        "            # 1. 解决复读问题\n",
        "            condition_on_previous_text=False,\n",
        "\n",
        "            # 2. 英文引导提示 (防止开头幻觉)\n",
        "            initial_prompt=\"Hello, welcome to the stream. This is an English transcript.\",\n",
        "\n",
        "            # 3. 提速优化\n",
        "            temperature=0.0,\n",
        "            vad_filter=True,\n",
        "            vad_parameters=dict(min_silence_duration_ms=500)\n",
        "        )\n",
        "\n",
        "        text_result = \"\".join([segment.text for segment in segments])\n",
        "\n",
        "        process_time = time.time() - start_time\n",
        "        log(f\"耗时: {{process_time:.2f}}s | 结果: {{text_result}}\")\n",
        "        return JSONResponse(content={{\"text\": text_result.strip()}})\n",
        "    except Exception as e:\n",
        "        log(f\"❌ 推理出错: {{e}}\")\n",
        "        return JSONResponse(content={{\"error\": str(e)}}, status_code=500)\n",
        "    finally:\n",
        "        if os.path.exists(temp_filename):\n",
        "            os.remove(temp_filename)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if not NGROK_AUTH_TOKEN:\n",
        "        log(\"❌ 错误：Token 未设置\")\n",
        "    else:\n",
        "        ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "        try:\n",
        "            public_url = ngrok.connect(8000).public_url\n",
        "            log(f\"\\\\n======== 英文转写服务已启动 ========\")\n",
        "            log(f\"API 接口地址: {{public_url}}/transcribe\")\n",
        "            log(f\"==================================\\\\n\")\n",
        "            uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "        except Exception as e:\n",
        "            log(f\"❌ 启动失败: {{e}}\")\n",
        "\"\"\"\n",
        "\n",
        "with open(\"server_main.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(server_code)\n",
        "\n",
        "print(\"已生成英文版脚本，正在启动...\", flush=True)\n",
        "\n",
        "env = os.environ.copy()\n",
        "env[\"PYTHONUNBUFFERED\"] = \"1\"\n",
        "\n",
        "process = subprocess.Popen(\n",
        "    [sys.executable, \"server_main.py\"],\n",
        "    env=env,\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True,\n",
        "    bufsize=1\n",
        ")\n",
        "\n",
        "try:\n",
        "    for line in process.stdout:\n",
        "        print(line, end=\"\")\n",
        "except KeyboardInterrupt:\n",
        "    print(\"服务已停止\")\n",
        "    process.terminate()"
      ],
      "metadata": {
        "id": "TUW5yIu5at-j",
        "outputId": "3a75da56-ba92-4870-9a44-43e0cde4190b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "正在安装依赖环境，请稍候...\n",
            "环境安装完成！\n",
            "已生成英文版脚本，正在启动...\n",
            "正在加载 Whisper 模型 (Tiny + float16)...\n",
            "✅ 模型加载完毕！\n",
            "Downloading ngrok ...\n",
            "Downloading ngrok: 0%\n",
            "Downloading ngrok: 1%\n",
            "Downloading ngrok: 2%\n",
            "Downloading ngrok: 3%\n",
            "Downloading ngrok: 4%\n",
            "Downloading ngrok: 5%\n",
            "Downloading ngrok: 6%\n",
            "Downloading ngrok: 7%\n",
            "Downloading ngrok: 8%\n",
            "Downloading ngrok: 9%\n",
            "Downloading ngrok: 10%\n",
            "Downloading ngrok: 11%\n",
            "Downloading ngrok: 12%\n",
            "Downloading ngrok: 13%\n",
            "Downloading ngrok: 14%\n",
            "Downloading ngrok: 15%\n",
            "Downloading ngrok: 16%\n",
            "Downloading ngrok: 17%\n",
            "Downloading ngrok: 18%\n",
            "Downloading ngrok: 19%\n",
            "Downloading ngrok: 20%\n",
            "Downloading ngrok: 21%\n",
            "Downloading ngrok: 22%\n",
            "Downloading ngrok: 23%\n",
            "Downloading ngrok: 24%\n",
            "Downloading ngrok: 25%\n",
            "Downloading ngrok: 26%\n",
            "Downloading ngrok: 27%\n",
            "Downloading ngrok: 28%\n",
            "Downloading ngrok: 29%\n",
            "Downloading ngrok: 30%\n",
            "Downloading ngrok: 31%\n",
            "Downloading ngrok: 32%\n",
            "Downloading ngrok: 33%\n",
            "Downloading ngrok: 34%\n",
            "Downloading ngrok: 35%\n",
            "Downloading ngrok: 36%\n",
            "Downloading ngrok: 37%\n",
            "Downloading ngrok: 38%\n",
            "Downloading ngrok: 39%\n",
            "Downloading ngrok: 40%\n",
            "Downloading ngrok: 41%\n",
            "Downloading ngrok: 42%\n",
            "Downloading ngrok: 43%\n",
            "Downloading ngrok: 44%\n",
            "Downloading ngrok: 45%\n",
            "Downloading ngrok: 46%\n",
            "Downloading ngrok: 47%\n",
            "Downloading ngrok: 48%\n",
            "Downloading ngrok: 49%\n",
            "Downloading ngrok: 50%\n",
            "Downloading ngrok: 51%\n",
            "Downloading ngrok: 52%\n",
            "Downloading ngrok: 53%\n",
            "Downloading ngrok: 54%\n",
            "Downloading ngrok: 55%\n",
            "Downloading ngrok: 56%\n",
            "Downloading ngrok: 57%\n",
            "Downloading ngrok: 58%\n",
            "Downloading ngrok: 59%\n",
            "Downloading ngrok: 60%\n",
            "Downloading ngrok: 61%\n",
            "Downloading ngrok: 62%\n",
            "Downloading ngrok: 63%\n",
            "Downloading ngrok: 64%\n",
            "Downloading ngrok: 65%\n",
            "Downloading ngrok: 66%\n",
            "Downloading ngrok: 67%\n",
            "Downloading ngrok: 68%\n",
            "Downloading ngrok: 69%\n",
            "Downloading ngrok: 70%\n",
            "Downloading ngrok: 71%\n",
            "Downloading ngrok: 72%\n",
            "Downloading ngrok: 73%\n",
            "Downloading ngrok: 74%\n",
            "Downloading ngrok: 75%\n",
            "Downloading ngrok: 76%\n",
            "Downloading ngrok: 77%\n",
            "Downloading ngrok: 78%\n",
            "Downloading ngrok: 79%\n",
            "Downloading ngrok: 80%\n",
            "Downloading ngrok: 81%\n",
            "Downloading ngrok: 82%\n",
            "Downloading ngrok: 83%\n",
            "Downloading ngrok: 84%\n",
            "Downloading ngrok: 85%\n",
            "Downloading ngrok: 86%\n",
            "Downloading ngrok: 87%\n",
            "Downloading ngrok: 88%\n",
            "Downloading ngrok: 89%\n",
            "Downloading ngrok: 90%\n",
            "Downloading ngrok: 91%\n",
            "Downloading ngrok: 92%\n",
            "Downloading ngrok: 93%\n",
            "Downloading ngrok: 94%\n",
            "Downloading ngrok: 95%\n",
            "Downloading ngrok: 96%\n",
            "Downloading ngrok: 97%\n",
            "Downloading ngrok: 98%\n",
            "Downloading ngrok: 99%\n",
            "Downloading ngrok: 100%\n",
            "                                                                                                    \n",
            "Installing ngrok ... \n",
            "                                                                                                    \n",
            "======== 英文转写服务已启动 ========\n",
            "API 接口地址: https://unavoidable-amber-plinthlike.ngrok-free.dev/transcribe\n",
            "==================================\n",
            "\n",
            "INFO:     Started server process [831]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
            "耗时: 1.16s | 结果:  A lot of people starting out in their taro journey don't like to read river source. They like to have all the cards upright and to read like that because I guess it's a little bit less.\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 0.25s | 结果:  Scary, but actually reading with the vessels makes the tower own a lot richer. Not only why the tower has 78 cards.\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 0.21s | 结果:  It says if you were speaking a language with just 78 words, allowing reverse ors gives you the luxury of enriching your language.\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 0.23s | 结果:  By doubling its vocabulary, some people suggested a reversal should mean not that. So if you got the sum, it means not the sum. So if you got the sum.\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 0.20s | 结果:  Appright, it means health, joy, happiness. And if you got the sun reversed, then it means the absence of health.\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 0.24s | 结果:  Illness may be you right? Disappointment, sadness and things like this, but I think we need to think about it in a slightly more subtle way than that because there are already cards that exist.\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 0.22s | 结果:  To signify sadness, disappointment, illness and many others. So I like to think about it in the sense of there could be a problem around this topic.\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 0.22s | 结果:  Another meaning that I like is this could be coming to an end or possibly this is being prevented. Now if you're not using reversals.\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 0.25s | 结果:  How can you know when there's a problem around a particular situation? You have to guess you have to use your intuition and that's fine, but you could simply use reversals and guitar accast to tell you the story themselves.\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 0.24s | 结果:  After all the whole point of doing divination is to get a heads up on what's coming up, so that we can address whatever it is that maybe a problem is all that's coming up.\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 0.28s | 结果:  There is just joy and happiness and health and love and their abundance and so on. Then you don't really need to know about it. You can just wait for it and it'll be wonderful.\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 0.24s | 结果:  The point is to get heads up on things that we might be missing. A lot of people starting out in their taro journey don't like to read river sals. They like to have all the cards up.\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "耗时: 0.45s | 结果:  I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry\n",
            "INFO:     64.181.228.168:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n",
            "服务已停止\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "欢迎使用 Colab",
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}