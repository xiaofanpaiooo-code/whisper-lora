# ==============================================================================
# æ¯•ä¸šè®¾è®¡æ ¸å¿ƒä»£ç ï¼šåŸºäº LoRA çš„ Whisper è¯­éŸ³è¯†åˆ«å¾®è°ƒç³»ç»Ÿ (æœ€ç»ˆä¿®æ­£ç‰ˆ)
# é€‚ç”¨ç¯å¢ƒï¼šGoogle Colab (T4 GPU)
# ä½œè€…ï¼šGemini åä½œå®Œæˆ
# ==============================================================================

# ------------------------------------------------------------------------------
# ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒé…ç½® (è¿è¡Œåè¯·é‡å¯ä¼šè¯)
# ------------------------------------------------------------------------------
# 1. å¸è½½å¯èƒ½å†²çªçš„åº“ï¼Œé”å®š datasets ç‰ˆæœ¬ä»¥è§£å†³ torchcodec è§£ç æŠ¥é”™
!pip uninstall -y datasets
!pip install -q datasets==2.21.0
!pip install -q transformers peft accelerate evaluate jiwer librosa soundfile bitsandbytes

print("âœ… ä¾èµ–å®‰è£…å®Œæ¯•ï¼è¯·ç‚¹å‡»èœå•æ  'ä»£ç æ‰§è¡Œç¨‹åº' -> 'é‡å¯ä¼šè¯'ï¼Œç„¶åç»§ç»­è¿è¡Œã€‚")

# ------------------------------------------------------------------------------
# ç¬¬äºŒæ­¥ï¼šæ ¸å¿ƒåº“å¯¼å…¥ä¸ç¯å¢ƒæ£€æŸ¥
# ------------------------------------------------------------------------------
import torch
import os
import random
from datasets import load_dataset, Audio
from transformers import (
    WhisperProcessor, 
    WhisperForConditionalGeneration, 
    Seq2SeqTrainingArguments, 
    Seq2SeqTrainer
)
from peft import LoraConfig, get_peft_model
from dataclasses import dataclass
from typing import Any, Dict, List, Union

# ç¡®ä¿ GPU å·²å¼€å¯
if not torch.cuda.is_available():
    print("âŒ è­¦å‘Šï¼šæœªæ£€æµ‹åˆ° GPUï¼Œè®­ç»ƒå°†æå…¶ç¼“æ…¢ï¼è¯·æ£€æŸ¥è¿è¡Œæ—¶è®¾ç½®ã€‚")
else:
    print(f"âœ… GPU å°±ç»ª: {torch.cuda.get_device_name(0)}")

# ------------------------------------------------------------------------------
# ç¬¬ä¸‰æ­¥ï¼šæ•°æ®åŠ è½½ (ä½¿ç”¨å…¬å¼€æ— é—¨æ§›çš„ Minds14 æ•°æ®é›†)
# ------------------------------------------------------------------------------
model_id = "openai/whisper-small" # æ¯•è®¾æ¨èä½¿ç”¨ small æ¨¡å‹
processor = WhisperProcessor.from_pretrained(model_id, language="Chinese", task="transcribe")

print("æ­£åœ¨åŠ è½½æ•°æ®é›† (polyai/minds14 ä¸­æ–‡å­é›†)...")
# å…³é—­ streaming æ¨¡å¼ä»¥é¿å…ç”±äºæ•°æ®é‡ä¸è¶³å¯¼è‡´çš„ ValueError
dataset = load_dataset("polyai/minds14", "zh-CN", split="train", streaming=False)

def prepare_dataset(batch):
    audio = batch["audio"]
    # æå– Log-Mel ç‰¹å¾
    batch["input_features"] = processor.feature_extractor(
        audio["array"], 
        sampling_rate=audio["sampling_rate"]
    ).input_features[0]
    # Minds14 æ–‡æœ¬å­—æ®µåä¸º 'transcription'
    batch["labels"] = processor.tokenizer(batch["transcription"]).input_ids
    return batch

# å¼ºåˆ¶é‡é‡‡æ ·åˆ° 16kHz å¹¶æ˜ å°„é¢„å¤„ç†
dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
dataset = dataset.map(prepare_dataset, num_proc=1)

print(f"âœ… æ•°æ®å‡†å¤‡å®Œæ¯•ï¼Œå…±è®¡ {len(dataset)} æ¡æ ·æœ¬ã€‚")

# ------------------------------------------------------------------------------
# ç¬¬å››æ­¥ï¼šæ•°æ®æ•´ç†å™¨ (Data Collator)
# ------------------------------------------------------------------------------
@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        # å¡«å……éŸ³é¢‘è¾“å…¥
        input_features = [{"input_features": feature["input_features"]} for feature in features]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")

        # å¡«å……æ–‡æœ¬æ ‡ç­¾
        label_features = [{"input_ids": feature["labels"]} for feature in features]
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

        # å°† padding çš„éƒ¨åˆ†è®¾ä¸º -100 ä»¥åœ¨ Loss è®¡ç®—ä¸­å¿½ç•¥
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)
        
        # å‰”é™¤é¦–ä½çš„ bos æ ‡è®° (Whisper çš„æ ‡å‡†åšæ³•)
        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():
            labels = labels[:, 1:]

        batch["labels"] = labels
        return batch

data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)

# ------------------------------------------------------------------------------
# ç¬¬äº”æ­¥ï¼šæ³¨å…¥ LoRA æ—è·¯çŸ©é˜µ (æ ¸å¿ƒç®—æ³•)
# ------------------------------------------------------------------------------
print("æ­£åœ¨åŠ è½½ Whisper å¹¶æŒ‚è½½ LoRA é€‚é…å™¨...")
model = WhisperForConditionalGeneration.from_pretrained(model_id)

# æ˜¾å­˜ä¼˜åŒ–ï¼šå¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
model.config.use_cache = False 
model.gradient_checkpointing_enable()

# LoRA é…ç½®ï¼šåªé’ˆå¯¹ Query å’Œ Value æŠ•å½±å±‚è¿›è¡Œä½ç§©è‡ªé€‚åº”
config = LoraConfig(
    r=8,              # ç§©ï¼Œå†³å®šäº†çŸ©é˜µçš„å®½åº¦
    lora_alpha=32,    # ç¼©æ”¾å› å­
    target_modules=["q_proj", "v_proj"], 
    lora_dropout=0.05,
    bias="none"
)

model = get_peft_model(model, config)
# æ‰“å°å¯è®­ç»ƒå‚æ•°å æ¯”ï¼ˆè®ºæ–‡ä¸­çš„å…³é”®æ•°æ®æŒ‡æ ‡ï¼‰
model.print_trainable_parameters()

# ------------------------------------------------------------------------------
# ç¬¬å…­æ­¥ï¼šè®¾ç½®è®­ç»ƒå‚æ•°å¹¶å¯åŠ¨
# ------------------------------------------------------------------------------
training_args = Seq2SeqTrainingArguments(
    output_dir="./whisper-lora-final-result",
    per_device_train_batch_size=8,        
    gradient_accumulation_steps=1,
    learning_rate=1e-3,                   # LoRA é€šå¸¸éœ€è¦è¾ƒå¤§çš„å­¦ä¹ ç‡
    warmup_steps=10,
    max_steps=50,                         # æ¼”ç¤ºè®¾ä¸º 50ï¼Œå®é™…å¾®è°ƒå»ºè®®è®¾ä¸º 500-1000
    fp16=True,                            # T4 GPU å¿…é¡»å¼€å¯åŠç²¾åº¦ä»¥åŠ é€Ÿ
    logging_steps=10,
    save_steps=25,
    remove_unused_columns=False,
    label_names=["labels"],
    report_to=["tensorboard"],
    processing_class=processor.feature_extractor, # è§„é¿æ—§ç‰ˆå¼ƒç”¨è­¦å‘Š
)

trainer = Seq2SeqTrainer(
    args=training_args,
    model=model,
    train_dataset=dataset,
    data_collator=data_collator,
)

print("ğŸš€ è®­ç»ƒå¼€å§‹...")
trainer.train()

# ------------------------------------------------------------------------------
# ç¬¬ä¸ƒæ­¥ï¼šç»“æœä¿å­˜ä¸å¯è§†åŒ–éªŒè¯
# ------------------------------------------------------------------------------
# 1. ä¿å­˜æƒé‡
save_path = "final_bishe_lora_model"
model.save_pretrained(save_path)
print(f"ğŸ’¾ æƒé‡å·²ä¿å­˜è‡³: {save_path}")

# 2. æ¨ç†æµ‹è¯•å¯¹æ¯”
print("\nğŸ“Š éšæœºæ ·æœ¬éªŒè¯ç»“æœ:")
model.eval()
test_sample = random.choice(list(dataset))
input_features = torch.tensor(test_sample["input_features"]).unsqueeze(0).cuda()

with torch.no_grad():
    generated_ids = model.generate(input_features, language="Chinese")

pred_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
real_text = processor.decode(test_sample["labels"], skip_special_tokens=True)

print("-" * 30)
print(f"çœŸå®æ ‡ç­¾: {real_text}")
print(f"LoRAé¢„æµ‹: {pred_text}")
print("-" * 30)
